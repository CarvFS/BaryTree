#!/bin/bash
# Begin LSF Directives
#BSUB -P GEN135
#BSUB -W 1:00
#BSUB -nnodes 1
#BSUB -alloc_flags gpumps
#BSUB -J gpu_tree_testing
#BSUB -o gpu_tree_testing.%J
#BSUB -e gpu_tree_testing.%J

# -n is number of resource sets
# -a is number of MPI tasks per resource set
# -c is number of CPU cores per resource set (i.e., number of total threads!)
# -g is number of GPUs




N=1000000
THETA=0.7
ORDER=8
CLUSTERSIZE=4000          
BATCHSIZE=4000
 
KERNEL=0
KAPPA=0.0

NZ=1
NY=1
for NX in 1
do
#NY=$NX
#NZ=$NX
NP=$(($NX * $NY * $NZ))
echo $NP
SOURCES=/gpfs/wolf/proj-shared/gen135/BaryTree/randomPoints/S${N}_${NX}x_${NY}y_${NZ}z.bin
TARGETS=/gpfs/wolf/proj-shared/gen135/BaryTree/randomPoints/T${N}_${NX}x_${NY}y_${NZ}z.bin
OFFSETS=/gpfs/wolf/proj-shared/gen135/BaryTree/randomPoints/offsets${N}_${NX}x_${NY}y_${NZ}z.bin
DIRECT=/gpfs/wolf/proj-shared/gen135/BaryTree/randomPoints/direct_gpu_${N}.bin
OUTPUT=/gpfs/wolf/proj-shared/gen135/BaryTree/randomPoints/tree_gpu_${N}.csv
jsrun -n ${NP} -a 1 -c 1 -g 1 ~/.local/bin/tree-distributed-gpu $SOURCES $TARGETS $OFFSETS $OFFSETS $DIRECT $OUTPUT $N $N $THETA $ORDER $CLUSTERSIZE $BATCHSIZE $KERNEL $KAPPA
done



